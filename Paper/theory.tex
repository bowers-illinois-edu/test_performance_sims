\documentclass[11pt]{article}

\usepackage{amssymb,amsmath,lscape,amsthm}
\usepackage[ruled]{algorithm2e}
\usepackage[T1]{fontenc}
\usepackage{libertine}
\usepackage[scaled=0.83]{FiraSans}
\usepackage[scaled=0.83]{FiraMono}
\usepackage{upquote}
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
\usepackage{parskip}
\usepackage{fancyvrb}
\usepackage{hyperref}
\hypersetup{pdfborder={0 0 0},
            breaklinks=true}

\usepackage[dvipsnames]{xcolor}
\usepackage[margin=1.5in]{geometry}
\usepackage{graphicx,grffile}
\usepackage{float} % for the H option
\usepackage{lscape}
\usepackage{pdflscape}

\setlength{\emergencystretch}{3em}  % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\setcounter{secnumdepth}{4}

% set default figure placement to htbp
% \makeatletter
% \def\fps@figure{htbp}
% \makeatother

%\usepackage[apaciteclassic]{apacite}
%\bibliographystyle{apacite}
\usepackage[style=authoryear, backend=biber]{biblatex}
%\usepackage{biblatex}
%\addbibresource{tmp.bib}
%\addbibresource{/Users/jwbowers/repos/Research-Group-Bibliography/big.bib}
\addbibresource{../BIB/references.bib}

\title{Why does the top-down tree-shaped hypothesis testing procedure control family wise error rates?}
\providecommand{\subtitle}[1]{}
\subtitle{Draft. Do not circulate or cite without permission. Comments welcome.}
\author{Jake Bowers and David Kim and \ldots}
\date{\today}

% For tables
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[table]{xcolor}
\usepackage{array}
\newcolumntype{G}{@{}>{\centering\arraybackslash}p{6ex}@{}}

\newcommand{\gta}{$>\!\alpha$}
\newcommand{\gga}{$\gg\!\alpha$}
\newcommand{\ggga}{$\ggg\!\alpha$}
\newcommand{\lea}{$\le\!\alpha$}
\newcommand{\leab}{\cellcolor{lightgray}\lea}

\usepackage{tikz}
%\usepackage{pgfplots}
\usepackage{mathtools}
\usepackage{tikz-cd}
\usetikzlibrary{arrows,automata,positioning,trees,fit,graphs,ext.nodes,ext.node-families}
\usepackage{forest}

% for the sparklines maybe skip this

\usepackage{sparklines}
% The height of the sparklines in ex units
\renewcommand\sparklineheight{1.75}
% The line width
\setlength\sparklinethickness{0.4pt}
% The color of the sparkline
\definecolor{sparklinecolor}{named}{blue}
% The color of the sparkine rectangle when present
\definecolor{sparkrectanglecolor}{gray}{0.8}
% The dot width
\setlength\sparkdotwidth{2pt}
% The color of the spikes
\definecolor{sparkspikecolor}{named}{red}
% The color of the bottom line when present
\definecolor{bottomlinecolor}{gray}{0.2}
% The thickness of the bottom line
\setlength\sparkbottomlinethickness{.2pt}
% The clipping separation (need sparklines v1.7 or later)
\setlength\sparklineclipsep{1pt}

\newcommand{\pfn}{\text{pfn}}
\newcommand{\bset}{\mathcal{B}}
\newcommand{\splitfn}{\text{splitfn}}
\newcommand{\bZ}{\bm{Z}}
\newcommand{\bS}{\bm{S}}
\newcommand{\bz}{\bm{z}}
\newcommand{\bOne}{\bm{1}}
\newcommand{\hyp}[1]{H_{\bset_{#1}}}
\newcommand{\p}[1]{p_{\bset_{#1}}}
%% Trying a way to include comments.
\usepackage[todonotes={textsize=footnotesize}]{changes}
%\usepackage{todonotes}
\definechangesauthor{JB}

\usepackage{listings}
\lstset{
  language=R,                     % the language of the code
  basicstyle=\normalsize\ttfamily, % the size of the fonts that are used for the code
  numbers=left,                   % where to put the line-numbers
  numberstyle=\tiny\color{NavyBlue},  % the style that is used for the line-numbers
  stepnumber=1,                   % the step between two line-numbers. If it is 1, each line
                                  % will be numbered
  numbersep=5pt,                  % how far the line-numbers are from the code
  backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
  showspaces=false,               % show spaces adding particular underscores
  showstringspaces=false,         % underline spaces within strings
  showtabs=false,                 % show tabs within strings adding particular underscores
  frame=single,                   % adds a frame around the code
  rulecolor=\color{black},        % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  tabsize=2,                      % sets default tabsize to 2 spaces
  captionpos=b,                   % sets the caption-position to bottom
  breaklines=true,                % sets automatic line breaking
  breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
  keywordstyle=\color{NavyBlue},      % keyword style
  commentstyle=\color{OliveGreen},   % comment style
  stringstyle=\color{OliveGreen}      % string literal style
}


\tikzcdset{arrow style=tikz, diagrams={>=stealth}}

\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}

\begin{document}
\maketitle

This section explains the theory behind the method and shows simulation results
that illustrate the theory.

\section{Recap of the method and notation}

Imagine that we have hypotheses ordered in an inverted tree-like structure as
shown in Figure~\ref{fig:simp_alg}.  We use this ordering to impose a
\textbf{stopping rule} and a \textbf{monotonicity rule}.\footnote{We combine
the \autocite{goeman2010sequential}, \autocite{goeman2012inheritance}, and
\autocite{rosenbaum2008a} ideas.} We show below that these rules allow weak
control of the family wise error rate (FWER) of testing the multiple hypotheses
represented in the tree --- control of the FWER when all of the hypotheses in
the tree are true. Then we show that these rules are not sufficient for strong
control of the FWER and add two rules which allow such control.



\begin{figure}[h]
\centering
\begin{tikzcd}[column sep=2em,
  row sep=small,
  every label/.append style={font=\footnotesize},
%  /tikz/execute at end picture={
%    \node (large) [rectangle, draw, fit=(H0)] {};
%}
  ]
	&& |[alias=H0]| {H_{\bset_{0}}} & {\text{Stop}} \\
	&& {\text{Split and Test}} \\
	{\text{Stop}} & {H_{\bset_{1}}} && {H_{\bset_{2}}} & {\text{Stop}} \\
	& {\text{Split and Test}} && {\text{Split and Test}} \\
	{H_{\bset_{3}}} & {H_{\bset_{4}}} && {H_{\bset_{5}}} & {H_{\bset_{6}}}
	\arrow[from=1-3, to=1-4, "{\text{if } p_{\bset_{0}} > \alpha}",]
	\arrow[from=1-3, to=2-3, "{\text{if } p_{\bset_{0}} \le \alpha}",]
	\arrow[from=2-3, to=3-2]
	\arrow[from=2-3, to=3-4]
	\arrow[from=3-2, to=3-1,"{p_{\bset_{1}} > \alpha}",above,sloped]
	\arrow[from=3-2, to=4-2,"{p_{\bset_{1}} \le \alpha}"]
	\arrow[from=3-4, to=3-5,"{p_{\bset_{2}} > \alpha}"]
	\arrow[from=3-4, to=4-4,"{p_{\bset_{2}} \le \alpha}"]
	\arrow[from=4-2, to=5-1]
	\arrow[from=4-2, to=5-2]
	\arrow[from=4-4, to=5-4]
	\arrow[from=4-4, to=5-5]
\end{tikzcd}

  \caption{The structured approach with fixed false positive rate $\alpha$. All
    blocks are in set $\bset_0$. All other hypotheses are subsets of those
    higher on the path and disjoint from others not on the same path. For
    example, $\bset_1 \subset \bset_0$ and $\bset_1 \cap \bset_2=\emptyset$.
    The $p$-value, $p_0$, is the result from a test of the hypothesis of no
    effects using all the blocks, $p_1$ is the $p$-value from a test using only
    the blocks in $\bset_1$. Testing stops when $p > \alpha$ or when the number
    of blocks in a given set of blocks $\bset$, $|\bset|$, is
  1.}\label{fig:simp_alg}

\end{figure}
%https://q.uiver.app/#q=WzAsMTMsWzIsMCwiSF97XFxic2V0X3swfX0iXSxbMSwyLCJIX3tCX3sxfX0iXSxbMywyLCJIX3tCX3syfX0iXSxbMCw0LCJIX3tCX3sxMX19Il0sWzEsNCwiSF97Ql97MTJ9fSJdLFszLDQsIkhfe0JfezIxfX0iXSxbNCw0LCJIX3tCX3syMn19Il0sWzIsMSwiXFx0ZXh0e1NwbGl0fSJdLFszLDAsIlxcdGV4dHtTdG9wfSJdLFswLDIsIlxcdGV4dHtTdG9wfSJdLFsxLDMsIlxcdGV4dHtTcGxpdH0iXSxbMywzLCJcXHRleHR7U3BsaXR9Il0sWzQsMiwiXFx0ZXh0e1N0b3B9Il0sWzAsNywiXFx0ZXh0e2lmIH0gcF97XFxic2V0X3swfX0gXFxsZSBcXGFscGhhIiwyXSxbNywxXSxbNywyXSxbMiwxMl0sWzIsMTFdLFsxLDldLFsxMCwzXSxbMTAsNF0sWzExLDVdLFsxMSw2XSxbMSwxMF0sWzAsOCwiXFx0ZXh0e2lmIH0gcF97XFxic2V0X3swfX0gPiBcXGFscGhhIl1d&macro_url=https%3A%2F%2Fgist.githubusercontent.com%2Fjwbowers%2Fc985bd71a3ec90b4e3d7e82595a8d6db%2Fraw%2F1ff36ee70583a2289e25eaa5d4bfe9fbdd51dcbd%2Fgistfile1.txt

We explain the rules here and then use them in propositions 1 and 2 and their
related proofs and simulation studies, below.

\paragraph*{Notation} We use $\bset$ to refer to the set of blocks relevant to
a given hypothesis. And write $\hyp{0}$ to refer to the hypothesis of no
effects focused on all blocks and units, $\hyp{1}$ to refer to the hypothesis
of no effects focused on the blocks and units in the subset $\bset_1$. Each
subset of blocks in the $k$-ary tree with $n$ nodes  $\bset_0, \ldots, \bset_{n}$ is (1) a subset of the whole, (2) a subset
of the other sets higher on the tree on the same path (for example $\bset_{2}
\subset \bset_{1} \subset \bset_0$), and (3) is disjoint from sets not on the
same path (for example $\bset_1 \cap \bset_2 = \emptyset$ and $\bset_{3} \cap
\bset_{4} = \emptyset$). Tests of hypotheses produce $p$-values, and those
$p$-values are labeled with the set to which they refer (so $\p{0}$ is the
$p$-value for the test of the hypothesis focused on $\bset_0$, and $\p{1}$ is
the $p$-value for the test focusing on those units and blocks in the set
$\bset_{1}$).


\subsection*{Rules of the method}

We list here all of the rules needed for both weak and strong control of the FWER.

\paragraph*{Rules sufficient for weak control of the FWER} We will show below
that the following three rules suffice for weak control of the FWER. Notice
that these rules do not involve data splitting.

\begin{description}

  \item[The \textbf{stopping rule}] instructs us to only
    test $\hyp{1}$ after rejecting $\hyp{0}$ and only test $\hyp{3}$ after
    rejecting $\hyp{1}$ (and thus after rejecting \emph{both} of the hypotheses
    that precede $\hyp{3}$ on that path of $\hyp{0} \rightarrow \hyp{1}
    \rightarrow \hyp{3}$). Any given hypothesis can only be tested
    if all its ancestor hypotheses on a path have been rejected. We stop
    testing once a hypothesis has not been rejected or once the path ends with
    a single block or node with no descendents (a ``leaf'' of the
    tree).

\item[The \textbf{monotonicity rule}] tells us that even if all ancestor
  hypotheses have been rejected, the critical value ($\alpha$) at which we
    reject a given hypothesis must be less than or equal to the critical value
    used in rejecting the ancestor hypotheses. In practice, with $\alpha$
    fixed, following this rule amounts to a restriction that $p$-values never
    decrease along a path.(This is like the \textcite{goeman2010sequential}
    ``monotonicity'' assumption). This rule reduces the power of tests as the
    tests descend the tree.


\item[The \textbf{false positive rate rule}]  In the case of fixed $\alpha$ we
  require that all tests at individual nodes, if executed on their own, have a
    false positive rate no more than $\alpha$. We can tend to justify this
    assumption because we could use direct permutations of randomized
    assignment to calculate these $p$-values, and this process plus our test
    statistic (which is effect increasing) should yield this kind of control
    \autocite[section TODO]{rosenbaum2002book}. (This is like the
    \textcite{goeman2010sequential} "single-step" assumption).

\end{description}

\paragraph*{Additional rules required for strong control of the FWER}

We then show that strong control of the FWER requires one or more of the following rules. \textbf{This is work in progress}

\begin{description}

  \item[The \textbf{sample splitting rule}] Although one could organize
    hypotheses in order where each hypothesis is tested on the same number of
    units\footnote{say the hypotheses are about the effect of a treatment on an
    outcome that is an index of different components and the focus is on
    detecting effects on individual components} in this case, we require that
    the sample size at each node be strictly less than the sample size of its
    parent. This maps directly onto the application --- where we are trying to
    separate blocks or groups of blocks within which the treatment effect is
    detectable from those within which it is not detectable. In practice, the
    sample size goes down quickly. For example in a $k$-ary tree with $k=2$ nodes
    at each level below the root and $l=3$ levels, and splitting the sample
    into equal numbers of blocks at each level, we have, say, $B$ total blocks
    used for $\hyp{0}$, $B/2$ blocks used for each of $\hyp{1}$ and $\hyp{2}$,
    and $(B/2)/2=B/4$ for each of $\hyp{3}, \ldots, \hyp{6}$ and $(B/4)/2=B/8$
    for the nodes at the next level. This also implies limits on the sizes of
    the tree, with $B=100$, a tree with $k=2$ can have no more than
    $l=5$.\footnote{We know that the number of nodes in a complete $k$-ary tree
    is $(k^{l+1}-1)/(k-1)$ and the maximum number of levels in a tree with $k$
    nodes at each level with $B$ total nodes is $l = \left\lfloor \log_k \left(
    B (k - 1) + 1 \right) \right\rfloor - 1$.}

    That is, this rule is built into the process of the application. We have to
    do some data splitting in order to  go from a single overall test of the
    null hypothesis of no effects for any unit in any block to tests of that
    same hypothesis in a single block.

    \textbf{We speculate} that one can control the FWER in a strong sense (with non
    trivial power) without data splitting, say, if the aim is to understand the
    contributions of a given test item to an overall score. But we focus on our
    specific case of block randomized experiments here, in which data splitting
    is an inherent part of detecting effects in subsets of and/or individual
    experimental blocks.

  \item[The \textbf{local adjustment rule}] When some nodes have non-null
    effects (or are ancestors of leaves with non-null effects) then the gating
    imposed by the stopping rule is not enough to control the FWER given the
    multiple tests in parallel at a given level of the tree. That said, we do
    not need to adjust all of the $k^l$ tests at a given level given the other
    rules. Instead we can make an adjustment to the tests within each parent.
    So, in the above figure, we would make an adjustment for testing both
    $\hyp{1}$ and $\hyp{2}$, and then separately we would adjust  the
    $p$-values arising from the two tests of $\hyp{3}$ and $\hyp{4}$ and again
    the two tests of $\hyp{5}$ and $\hyp{6}$. In our application we use the
    Hommel adjustment for each of the $k$ hypotheses at a given level but one
    could also use a more conservative adjustment.\footnote{We think that this
    adjustment is conservative. We leave for another project the derivation of
    the optimal local adjustment. In what follows we compare the Hommel
    adjustment for $p$-values across $k$ hypotheses to approaches that directly
    adjust the $\alpha$ level for the hypotheses at a given level.} \textbf{So
    far, our simulations show that local FWER adjustments across all $k$ nodes
    of a given parent do control the FWER in a strong sense when combined with
    the stopping rule, monotonicity, valid false positive rate, and data
    splitting.}
    %TODO: Run a subset of the simulations varying power and also adding local fdr.

  \item[The \textbf{global adjustment rule}] \textbf{We speculate} that it is
    possible that we can skip the local adjustment rule and just do a global
    adjustment FWER or even FDR adjustment for the $p$-values that are less
    than or equal to $\alpha$ in the tree after all of the nodes have been
    tested following the other rules. We may leave this rule for a different
    paper but we explored it in the simulations that we show
    below.\footnote{TODO: Check that when we do this the results respect the
    rules. For example, that monotonicity and gating are respected.}

\end{description}

We also speculate that we can increase power while still controlling the FWER
by combining these rules with approaches that "invest" or "spend" false
positive error by adjusting the rejection threshold, $\alpha$, for the child
nodes or a given parent.

\section{Weak Control of the FWER using the rules}

We show here that the method described above controls the FWER when all of the
hypotheses are true. In fact, the proof that we show below does not require the
sample splitting step --- so that weak control can be shown to hold for
hypotheses organized into a tree-like structure even without the sample
reduction at each level of the tree, for example for covariates gathered into
indices, or other multi-item scores.


\begin{proposition}{The three rules suffice to control FWER all hypotheses are true}\label{prop:weakctrl}

  A family of hypotheses with individually controlled false positive rates, such
  as permutation tests from a randomized experiment, organized on a $k$-ary
  tree and tested following the stopping rule with a fixed $\alpha$ and
  monotonicity rules will produce no more than $\alpha$ false positive errors
  across the whole tree. That is, a family of hypotheses tested in this way
  will weakly control the FWER.

\end{proposition}


\begin{proof}{Proof of Proposition~\ref{prop:weakctrl}}\label{proof:prop_weakctrl}

  \paragraph*{Step 1. Control at the Root}

  Let $p_0$ be the p-value at the root (level 0). Because the test is valid by
  the \textbf{false positive rate rule}, we have $\Pr(p_0 \le \alpha) \le
  \alpha$.

  If $p_0 > \alpha$, the procedure stops immediately and no further tests occur
  because of the \textbf{stopping rule}. Therefore, any false rejection in the
  tree can occur only if $p_0 \le \alpha$.

  \paragraph*{Step 2. Consequence of the stopping rule.}

  Because of the stopping rule, a hypothesis at any level is tested only
  if all its ancestor hypotheses had $p$-values $\le \alpha$. This means that
  the event that any node in the tree is tested is a subset of the event of a
  false rejection at the root node, $\{p_0 \le \alpha\}$. Write the event
  of at least one false rejection on the tree as as $E$. The stopping rule
  makes this event a subset of the event of a false rejection at the
  root so $E \subseteq \{p_0 \le \alpha\}$.

  \paragraph*{Step 3. The law of total probability}

  Now, let’s use the law of total probability. We partition the sample space into
  two disjoint events: $A = \{p_0 \le \alpha\}$ (the root test is significant,
  so testing continues), $B = \{p_0 > \alpha\}$ (the root test is not
  significant, so no further tests occur).

  By the law of total probability, we have $\Pr(E) = \Pr(E \mid A)\Pr(A) + \Pr(E \mid B)\Pr(B)$.


  Notice that if $p_0 > \alpha$ (i.e. event $B$ occurs), then no tests beyond the
  root are performed and hence no false rejection can occur. Thus, $\Pr(E \mid
  B) = 0$.

  This simplifies our equation to $\Pr(E) = \Pr(E \mid A)\Pr(A)$.

  Since probabilities are bounded by 1, we have $\Pr(E \mid A) \le 1$ and because
  the test at the root is valid, we know $\Pr(A) = \Pr(p_0 \le \alpha) \le
  \alpha$.

  Combining these facts we obtain $\Pr(E) \le 1 \cdot \Pr(A) \le \alpha$.

  Thus, regardless of how many nodes or branches are tested later on, if $p_0 >
  \alpha$ then no tests beyond the root occur, and no false rejections are
  possible. At this point, we have only used the stopping rule and the false
  positive rule.

  \paragraph*{Step 3. Analysis Along a Branch}

  This result might seem strange. After all, we could have many tests at any
  level below the root (in fact $k^l$ at any given level). So, here we connect
  the results from the strictly nested testing approaches of
  \autocite{rosenbaum2008a} and \autocite{marcus1976closed} to the above.

  Suppose the root is rejected (i.e. $p_0 \le \alpha$); then the $k$ children
  at level $l=1$ are tested. Consider one branch from the root down through any
  number of nodes to a leaf. At the first level, for a given child, $\hyp{1}$,
  with parent’s p-value $p_0$, its conditional probability of being falsely
  rejected is $\Pr(p_{1} \le \alpha \mid p_0) = \frac{\alpha-p_0}{1-p_0} \le
  \alpha$.  We can write this probability as that fraction, because (1) $p_0
  \le \alpha$, (2) the \textbf{false positive rule} implies that all of the
  p-values in this tree of true null hypotheses are draws from a uniform
  distribution, and (3)  the \textbf{monotonicity rule} requires that $p_1 \ge
  p_0$.

  Similarly, for a node at level 2 along that branch, its test is only
  performed if the level 1 node was rejected. By the same logic, for example,
  with $\hyp{3}$ and it's parent $\hyp{1}$, $\Pr(p_{3} \le \alpha \mid
  p_1 \le \alpha) \le \alpha$, and so on.

  Thus, for any given branch, the probability that all tests along that branch
  yield rejections is at most the product of probabilities on the branch up to
  that point and since the probabilities are all less than $\alpha$, the
  product is (much) less than $\alpha$.

  \paragraph*{Conclusion}

  This reasoning shows that even though there may be $k$ children at level 1,
  $k^2$ at level 2, and so on, the overall probability that any false rejection
  occurs in the tree is bounded by the probability that the root is falsely
  rejected by the \textbf{stopping rule} and \textbf{monotonicity rule} and
  \textbf{false positive rule}. Since the root is tested only once and its
  rejection probability is at most $\alpha$, the procedure --- by gating
  further tests on the rejection of the root --- ensures that $\Pr(\text{at
  least one false rejection}) \le \alpha$.

  In summary, the law of total probability allows us to break down the overall
  error probability into two parts (depending on whether $p_0 \le \alpha$ or
  not), and since no testing occurs when $p_0 > \alpha$, the entire false
  positive error is controlled by the root’s rejection probability, which is at
  most $\alpha$.

  Thus, the entire procedure controls the FWER at level $\alpha$ under the
  complete null hypothesis.

\paragraph*{An alternative approach:}

Consider a tree with two levels and $k$ nodes per level. In this case we have
$p_0$ (at the root or first level) and then $p_1, \ldots, p_k$ at the second
level. What is the probability of making at least one false positive error
among these tests?

We might write this as $\Pr(\min(p_0,p_1,\ldots,p_k) \le \alpha)$. By the
\textbf{false positive rule} we know that $\Pr(p_0 \le \alpha) \le \alpha$. By
the \textbf{monotonicity rule} and \textbf{stopping rule} we know that
$\min(p_0,p_1,\ldots,p_k) = p_0$. So, $\Pr(\min(p_0,p_1,\ldots,p_k) \le
\alpha)=\Pr(p_0 \le alpha) \le \alpha$.


\end{proof}

\textbf{To Notice:} Although we described the procedure using the idea of data
splitting, this was not necessary in the proof.

\subsection*{Simulation Study}

We created a simple simulation study to illustrate this logic.

\paragraph*{Details of the simulation study}

Given a specification of a complete $k$-ary tree using $k$ nodes per level and
$L$ levels we drew $p$-values from uniform distributions following the rules:
\begin{enumerate}

  \item For the root node, $p_0 \sim U(0,1)$; a draw which respects the
    \textbf{false positive rule} such that $\Pr(p_0 \le \alpha) \le \alpha$ in
    this case.

    \item If $p_0 \le \alpha$, we drew $p_{l=1,k}$ independent $p$-values from
      $U(p_0,1)$ (implementing the \textbf{monotonicity rule}) otherwise we
      stopped testing.

    \item For any of the $k$ $p$-values at level 1 where $p_{l=1,k} \le
      \alpha$, we generated $p$-values for its children with $U(p_{\text{parent
      $p$}},1)$, implementing the \textbf{stopping rule}.

    \item We then continued testing, drawing from uniform distributions with
      lower bounds equal to the parent $p$-values for parents with $p \le
      \alpha$ as we descended into the tree towards the leaves, stopping
      testing in any branch when $p > \alpha$

\end{enumerate}

In this simulation we did not change the power of the test (i.e. representing
the idea of data splitting) other than to enforce the \textbf{stopping rule}
and \textbf{monotonicity rule}.

For each tree, we repeated this procedure 10,000 times, recording whether any
of the $p$-values in the tree of true null hypotheses were $\le \alpha$. The
proportion of simulations with at least one such false rejection is a measure
of the FWER.

\paragraph*{Weak FWER Control Simulation Results}
% see Simple_analysis/results_exploration.R
Table~\ref{tab:weak_control_sim} shows the results. Whether the tree has 2
nodes per level or 100, the rules lead to a maximum FWER within simulation
error. We calculate simulation error, with 10,000 simulations a rough measure
of simulation error is $2 \times \sqrt{.05 (1-.05)/10000}$ so we would expect
results as large as $.05 +.004 = .054$ given the variability arising from
simulation (We could also have chosen $2 \times \sqrt{.5 (1-.5)/10000}=.01$
with an upper bound of $.05+.01=.06$. TODO. Not sure which to choose.) Each
tree with $k$ nodes per level and a maximum of $L$ levels, starts with a single
root node and then has $k$ nodes at level 1, $k^2$ at level 2, and $k^L$ leaves
or terminal nodes.

\input{weak_control_sim_tab.tex}

Notice that the rules lead the algorithm to nearly always stop after testing
the single root node (which produces $p_0 \le .05$ in less than 5\% of the
simulations). This is true even when the trees have many nodes and many leaves.
In contrast the bottom-up procedure would test in each of the leaves and then
adjust the results of all of those tests. In this table we see that the
bottom-up procedure would be required to do thousands of tests: its FWER would
be controlled (not shown here). Power is not relevant in this case --- where
the null hypothesis of no effects is true for all nodes. We will show later,
when the hypothesis of no effects is false for some nodes, the requirement that
the bottom-up approach test each leaf severely reduces its power to detect any
effects at all.

\paragraph*{Why might weak control suffice?}

In many exploratory research scenarios analysts want to identify candidates for
future follow-up studies. Since strong control of the FWER tends to have lower
power than weak control, a researcher may prefer to only apply the three rules
that we describe under the idea that more true effects might be detected in
exchange for a slightly elevated risk of flagging false effects (since, as we
will show below, the three rules do not suffice in all cases to control the
FWER when some nodes contain non-null effects). In the case of the
tree-organized hypotheses, the risk is a function of the number of nodes at
each level of the tree $k$ and the number of levels of the tree $l$ as well as
the proportion of the leaves of the tree with non-null effects or false
hypotheses.


\section{Strong control of the FWER: More Rules}

\textbf{This section is in progress}

The three rules that allow control of the FWER in a weak sense do not control
it in a strong sense. For example, Table~\ref{tab:strong_control_naive} shows
the situation where half of the leaves of each tree have non-null causal
effects. By definition, if a leaf has a non-null effect, all its ancesters also
contain a non-null effect. An ideal hypothesis testing procedure should reject
the null of no effects for that leaf and all of its ancestors. This dependency
also means that whenever the null hypothesis of no effects is false for any
leaf of a tree it is also false for the root of the tree and for all nodes on
the path between the root and that leaf.

Our simulation procedure to assess strong control of the FWER differed from the
simulation described above in regards the construction of the $p$-values by
using a Beta distribution for non-null nodes in the tree to simulate a
relatively high powered test, by simulating data-splitting by adjusting one of
the Beta shape parameters, and by implementing different modes of adjustment
for the rejection thresholds (the $\alpha$) or the $p$-values.

\begin{enumerate}

  \item \textbf{Simulating high powered tests for non-null nodes}  Like the
    above simulations, we draw $p$-values from $\text{U}(p_\text{parent},1)$
    for nodes with no effects (the null is true).For the root node,
    $p_\text{parent}=0$.

    Since this tree has some nodes containing non-null effects, we draw
    $p$-values from a Beta distribution rescaled to go from $p_\text{parent}$
    as a lower limit to $1$ as an upper-limit for those non-null nodes: drawing
    one value from $p^*_{\text{non-null}} \sim \text{Beta}(a,1)$ and then
    rescaling to produce $p_{l,k}=p_\text{parent} + (1-p_\text{parent}) \cdot
    p^*_{\text{non-null}}$.

    We set $a=.1$ as the first shape parameter of the Beta distribution to
    simulate a powerful test: a Beta distribution with $a=.1$ and $b=1$ will
    produce about 74\% of $p$-values less than .05 (in R code we can see this
    with \lstinline[language=R]{pbeta(.05,.1,1)=.74}.

  \item \textbf{Simulating data-splitting} We simulated the idea of
    data-splitting by increasing the parameter of the beta distribution
    governing the production of $p$-values for nodes where the null hypothesis
    of no effects was false. For example, if $a=.1$, then power is roughly .74.
    If $a=.2$, then power is roughly .55. And if $a=.5$, then power is roughly
    .22. We created a function so that $a$ increased up to .5 smoothly and in a
    way that responded to the number of nodes at each level (so that, say, the
    decrease in power between levels is proportional to $k$). We set a lower
    limit on the power such that we never increased $a$ over $.5$ ($a=.5$ is
    equivalent to power $\approx$ .22).

    In actual practice we would split the data itself and stop testing and
    delineation of the tree once we could no longer split rather than execute
    tests on every node in a complete tree as we do in these simulations.

  \item \textbf{Local or Global Adjustments} We will describe these in detail
    below. Briefly, given the $p$-values for the children of a given parent
    node (after simulating data-splitting) we make adjustments to those
    $p$-values or to the threshold for rejection ($\alpha$) to further reduce
    the probability of a false positive. Our most successful idea involves a
    Hommel adjustment for all $k$ $p$-values of a given parent (not all $k^l$
    $p$-values of a given level of the tree). Other ideas included making no
    adjustments until after producing all of the $p$-values in the tree and
    then implementing a global adjustment. We implemented multiple approaches
    to changing the rejection threshold at a given node and provide the R code
    below:

\begin{lstlisting}[language=R]
  if (alpha_method == "fixed") {
    ## fixed \alpha
    child_threshold <- parent_alpha
  } else if (alpha_method == "spending") {
    ## This is not alpha-spending. Just using the term for now.
    ## Here if parent_p = .01 and alpha=.05, we have .05-.01 = .04 to
    ## divide among the k children so we would have (.05-.01)/4
    children_alpha_adjust <- max((alpha - parent_p), 0) / k
    child_threshold <- parent_alpha + children_alpha_adjust
  } else if (alpha_method == "investing") {
    ## This is not alpha-investing. Just using the term for now.
    children_alpha_adjust <- max((parent_alpha - parent_p), 0) / k
    child_threshold <- parent_alpha + children_alpha_adjust
  } else if (alpha_method == "fixed_k_adj") {
    child_threshold <- alpha / (1 + alpha * k)
  } else if (alpha_method == "adaptive_k_adj") {
    child_threshold <- parent_p + (1 - parent_p) * (alpha / k)
  } else {
    stop("alpha_method must be one of 'fixed', 'spending', or 'investing' or 'fixed_k_adj' or 'adaptive_k_adj'.")
  }
\end{lstlisting}
\end{enumerate}

\subsection{No general strong control using only the three rules}

Table~\ref{tab:strong_control_naive} shows that as the number of nodes per
level increase, the minimum FWER also increases above .05. 

\input{strong_control_naive_tab.tex}

This table also shows the effects of simulated data splitting as a kind of
local adjustment: the rows at the top of the table show less control of the
FWER and much less gating. In the output below we focus on the case with $k=2$
to describe what is happening in a bit more detail. We see that when $k=2$ and
$l=18$, the whole tree has 524,287 total nodes of which 262,144 are terminal
nodes or leaves. When \Verb+adj_effN+ is \texttt{FALSE} (no data splitting,
the test has the same power at all nodes), it visits, on average, across 10,000
simulations, 607.7 nodes out of the total and it makes at least one false
rejection 55\% of those simulations: the three rules do not control the FWER
here. At the bottom of the output table, we see the analogous line for $k=2$
and $l=18$ in which we simulate data-splitting by reducing the power of the
test at each level from .74 to .22 as described above. In that case, the rules
stop the testing after visiting no more than about 4 nodes (on average, across
the 10,000 simulations), no leaves are ever visited, and no false rejections
occur.

\begin{Verbatim}[fontsize=\footnotesize]
k  l adj_effN total_nodes num_nodes_tested false_error  power num_leaves_tested leaf_power num_leaves
2  2    FALSE           7           2.7354      0.0490 0.7410            0.6429          1          4
2  4    FALSE          31           5.7143      0.0751 0.7455            0.7358          1         16
2  6    FALSE         127          13.2254      0.1404 0.7386            1.1086          1         64
2  8    FALSE         511          26.7850      0.2081 0.7367            2.0409          1        256
2 10    FALSE        2047          55.5525      0.3244 0.7422            3.9855          1       1024
2 12    FALSE        8191         103.5610      0.4173 0.7401            7.0598          1       4096
2 14    FALSE       32767         191.1142      0.4811 0.7403           12.6456          1      16384
2 16    FALSE      131071         343.4361      0.5274 0.7498           21.7593          1      65536
2 18    FALSE      524287         607.7383      0.5519 0.7389           37.3772          1     262144
2  2     TRUE           7           2.5191      0.0452 0.7373            0.5693          1          4
2  4     TRUE          31           3.5107      0.0346 0.7364            0.2367          1         16
2  6     TRUE         127           4.1352      0.0097 0.7477            0.0266          1         64
2  8     TRUE         511           4.2031      0.0009 0.7456            0.0018          1        256
2 10     TRUE        2047           4.1435      0.0000 0.7380            0.0001          1       1024
2 12     TRUE        8191           4.1718      0.0000 0.7418            0.0000         NA       4096
2 14     TRUE       32767           4.1442      0.0000 0.7468            0.0000         NA      16384
2 16     TRUE      131071           4.1641      0.0000 0.7391            0.0000         NA      65536
2 18     TRUE      524287           4.1380      0.0000 0.7319            0.0000         NA     262144
\end{Verbatim}

These results suggest the need for some other adjustment to the testing
procedure above the gating and monotonicity requirements. The data splitting
approach (as simulated roughly here) reduces the power of the test and thus
protects the FWER when the number of nodes per level is 2 in this particular
simulation. But Table~\ref{tab:strong_control_tab} shows that data splitting as
implemented here is not enough.

\clearpage

\subsection{Strong control using local adjustments}

However, if we apply one of the additional adjustments to the testing
procedure, we  can control the FWER in a strong sense even with thousands of
potential tests.

%\input{strong_control_tab}


\paragraph*{Local Children Hommel Adjustment} Adjust the $p$-values for all of
the immediate children of a rejected parent. One approach involves two steps.
First, we take all $k$ children of a node that has passed the rejection gate,
"split" the sample, testing the hypothesis of no effects in each of them,
applying the monotonicity rule from above where $\min(p_{l,k})=p_\text{parent}$
for a node $k$ in level $l$, to produce $p^{*}_{l,k}$. Second, we apply the
Hommel adjustment across all $k$ $p$-values to produce $p^{adj}_{l,k}$.
Rejection/gate-passing decisisions for each node are then dependent on
$p^{adj}_{l,k}$. The below table shows strong control of the FWER in a
simulation designed for a situation where half of the leaves in each tree are
non-null, such that the root is always non-null, and using the Beta(.1,1)
distribution to represent the $p$-values that would  be generated by a
relatively powerful test in the presence of a non-null effect.

We notice the average proportion of nodes with false positives is always
bounded from above by .05. We also notice that the gating approach tends to
test many fewer nodes than the bottom up approach. But it tends to identify
nodes with non-null effects. So, its power to detect effects when it tests them
is high. In the extreme case, this approach nearly always rejects non-null
hypotheses in the few leaf nodes that it tests (minimum leaf power is 1), yet
on average the algorithm does not even test a single leaf during each
simulation.

\begin{Verbatim}[fontsize=\footnotesize]
    k min_l max_l max_fwer min_power max_nodes_tested min_leaf_power max_leaves_tested min_bottom_up_power max_leaves bottom_up_detected_leaves
    2     2    18   0.0327     0.738             3.29              1             0.478            0.000437     262144           57.3
    4     2     8   0.0317     0.730             2.91              1             0.451            0.000877      65536           28.7
    6     2     6   0.0346     0.739             2.73              1             0.412            0.00104       46656           24.2
    8     2     6   0.0317     0.732             2.70              1             0.455            0.000437     262144           57.3
   10     2     6   0.0296     0.741             2.64              1             0.454            0.000224    1000000          112.
   12     2     4   0.027      0.748             2.56              1             0.456            0.00156       20736           16.1
   14     2     4   0.0269     0.737             2.49              1             0.467            0.00114       38416           21.9
   16     2     4   0.0251     0.737             2.37              1             0.450            0.000876      65536           28.7
   18     2     4   0.0189     0.731             2.28              1             0.438            0.000693     104976           36.4
   20     2     4   0.0177     0.732             2.14              1             0.389            0.000558     160000           44.7
   50     2     2   0.0175     0.74              2.39              1             0.630            0.00450        2500            5.62
  100     2     2   0.0269     0.739             3.25              1             1.21             0.00225       10000           11.3
\end{Verbatim}

\paragraph*{Global Adjustment}

The idea here is to take the collection of $p$-values arising from the simple
application of the three rules and then adjust them all after those rules have
been applied. Any $p > \alpha$ would be not rejected and all of its children
would then be required to be not rejected: monotonicity of $p$-values is
already built into the algorithm, so an adjustment method like Hommell or Holm
would not need additional work.

\paragraph*{Local rejection threshold adjustment}

The idea here is to try to either (1) decrease the rejection threshold for each
test in a way that is less conservative than Hommell and/or (2) add power back
to the case with the Hommell adjustment.



%\autocite{miecznikowski2023error}
\subsection{Summary}

We see that not only does the method control FWER for fully ordered hypotheses,
it controls the FWER under a broad range of parallel testing scenarios. This
might be a serious limitation in genomics, where trees might be relatively
shallow (say, no more than 3 levels), but when the trees are created in a data
dependent manner (for example, if the number of tests is 2 at each level and
the tree can be much deeper than 3 levels) this suggests that for all practical
purposes our method controls the FWER.

\clearpage
\printbibliography


\end{document}
